<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>Live AI Voice Assist (OpenAI Realtime + Whisper)</title>
  <style>
    :root {
      --bg: #0f1115; --panel: #161a22; --text: #e6e9ef; --muted: #9aa4b2;
      --accent: #3b82f6; --border: #222837;
    }
    html, body { height: 100%; }
    body {
      font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Arial, sans-serif;
      margin: 0; background: radial-gradient(1200px 800px at 10% -20%, #1a2030 0, #0f1115 60%) fixed; color: var(--text);
    }
    .container { max-width: 980px; margin: 32px auto; padding: 0 20px; }
    h1 { margin: 0 0 10px; font-weight: 700; letter-spacing: .2px; }
    #controls { display: flex; align-items: center; gap: 10px; margin-bottom: 18px; }
    #controls button {
      padding: 10px 14px; border-radius: 10px; border: 1px solid var(--border);
      background: #1a2230; color: var(--text); cursor: pointer; transition: all .15s ease;
    }
    #controls button:hover { border-color: var(--accent); box-shadow: 0 0 0 3px rgba(59,130,246,.15) inset; }
    #controls button[disabled] { opacity: .5; cursor: not-allowed; }
    #status { color: var(--muted); font-size: 0.95em; margin-left: 6px; }
    .row { display: flex; gap: 18px; align-items: stretch; }
    .col { flex: 1; min-width: 280px; }
    .panel { background: var(--panel); border: 1px solid var(--border); border-radius: 12px; padding: 12px; min-height: 180px; box-shadow: 0 8px 30px rgba(0,0,0,.28); }
    .label { font-weight: 600; margin-bottom: 8px; color: #b6c2d0; }
    #userTranscript, #aiTranscript { white-space: pre-wrap; min-height: 130px; }
    .small { color: var(--muted); font-size: 12px; margin-top: 6px; }
    audio { width: 100%; margin-top: 6px; outline: none; }
    .muted { color: var(--muted); }
  </style>
</head>
<body>
  <div class="container">
    <h1>Live AI Voice Assist</h1>
    <div id="controls">
      <button id="startBtn">Start</button>
      <button id="stopBtn" disabled>Stop</button>
      <span id="status">Idle</span>
    </div>

    <div class="row">
      <div class="col">
        <div class="panel">
          <div class="label">You (live transcription)</div>
          <div id="userTranscript" class="muted">Speak after you click Startâ€¦</div>
        </div>
      </div>
      <div class="col">
        <div class="panel">
          <div class="label">Assistant (text)</div>
          <div id="aiTranscript" class="muted">The assistant will respond here.</div>
          <div class="small">Audio playback:</div>
          <audio id="aiAudio" autoplay playsinline></audio>
        </div>
      </div>
    </div>

    <div class="small" style="margin-top:12px;">
      Tip: Use Chrome/Edge on http://127.0.0.1:8000 for local mic/WebRTC. Ensure your OpenAI account has access to the realtime model.
    </div>
  </div>

<script>
(function(){
  const startBtn = document.getElementById("startBtn");
  const stopBtn  = document.getElementById("stopBtn");
  const statusEl = document.getElementById("status");
  const userEl   = document.getElementById("userTranscript");
  const aiEl     = document.getElementById("aiTranscript");
  const aiAudio  = document.getElementById("aiAudio");

  let pc = null;
  let micStream = null;
  let eventsDC = null;
  let sessionId = ""; // capture from /session to store with conversation
  const DEBUG_EVENTS = false;

  function setStatus(text){ statusEl.textContent = text; }
  function clearEl(el){ el.textContent = ""; el.classList.remove("muted"); }
  function append(el, text){ el.textContent += text; el.scrollTop = el.scrollHeight; }
  function appendLine(el, text){ append(el, (text || "") + "\n"); }

  function attachEventChannel(dc) {
    if (!dc) return;
    eventsDC = dc;
    eventsDC.onopen = () => setStatus("Connected (events channel open).");
    eventsDC.onclose = () => setStatus("Events channel closed.");
    eventsDC.onmessage = (ev) => {
      let evt;
      try { evt = JSON.parse(ev.data); } catch { return; }
      if (DEBUG_EVENTS) console.log("oai-event:", evt);
      handleRealtimeEvent(evt);
    };
  }

  function handleRealtimeEvent(evt) {
    const t = evt?.type || "";

    if (t.includes("transcription.delta")) {
      const text = extractText(evt);
      if (text) append(userEl, text);
      return;
    }
    if (t.includes("transcription.completed")) {
      appendLine(userEl, "");
      return;
    }

    if (t === "response.output_text.delta") {
      const text = extractText(evt);
      if (text) append(aiEl, text);
      return;
    }
    if (t === "response.output_text.done") {
      appendLine(aiEl, "");
      return;
    }

    if (t.startsWith("response.") && t.endsWith(".delta")) {
      const text = extractText(evt);
      if (text) append(aiEl, text);
      return;
    }
    if (t === "response.completed") {
      appendLine(aiEl, "");
      return;
    }
  }

  function extractText(evt) {
    if (!evt || typeof evt !== "object") return null;
    if (typeof evt.delta === "string") return evt.delta;
    if (evt.delta && typeof evt.delta.text === "string") return evt.delta.text;
    if (typeof evt.text === "string") return evt.text;
    if (typeof evt.transcript === "string") return evt.transcript;
    if (typeof evt.message === "string") return evt.message;
    if (evt.output_text && Array.isArray(evt.output_text)) return evt.output_text.join("");
    return null;
  }

  async function start() {
    startBtn.disabled = true;
    stopBtn.disabled = false;
    setStatus("Requesting microphone...");
    clearEl(userEl);
    clearEl(aiEl);

    try {
      micStream = await navigator.mediaDevices.getUserMedia({ audio: true });
    } catch (e) {
      setStatus("Microphone access denied.");
      startBtn.disabled = false;
      stopBtn.disabled = true;
      return;
    }

    setStatus("Creating session...");
    let session;
    try {
      const r = await fetch("/session/");
      if (!r.ok) throw new Error(await r.text());
      session = await r.json();
      sessionId = session?.id || ""; // capture for saving
      console.log("Session created:", sessionId);
    } catch (e) {
      console.error(e);
      setStatus("Failed to create session.");
      startBtn.disabled = false;
      stopBtn.disabled = true;
      return;
    }

    const ephemeralKey = session?.client_secret?.value;
    const model = session?.model || "gpt-4o-realtime-preview";
    if (!ephemeralKey) {
      console.error("Invalid session:", session);
      setStatus("Invalid session response.");
      startBtn.disabled = false;
      stopBtn.disabled = true;
      return;
    }

    setStatus("Starting WebRTC...");
    pc = new RTCPeerConnection({ iceServers: [{ urls: ["stun:stun.l.google.com:19302"] }] });

    pc.ondatachannel = (event) => {
      if (event.channel?.label === "oai-events") attachEventChannel(event.channel);
    };
    const proactiveDC = pc.createDataChannel("oai-events");
    attachEventChannel(proactiveDC);

    pc.ontrack = (event) => {
      const [stream] = event.streams;
      aiAudio.srcObject = stream;
    };

    for (const track of micStream.getTracks()) pc.addTrack(track, micStream);
    pc.addTransceiver("audio", { direction: "recvonly" });

    const offer = await pc.createOffer();
    await pc.setLocalDescription(offer);

    let answerSdp = "";
    try {
      const sdpResponse = await fetch(`https://api.openai.com/v1/realtime?model=${encodeURIComponent(model)}`, {
        method: "POST",
        headers: {
          "Authorization": `Bearer ${ephemeralKey}`,
          "Content-Type": "application/sdp",
          "OpenAI-Beta": "realtime=v1",
        },
        body: offer.sdp,
      });
      if (!sdpResponse.ok) throw new Error(await sdpResponse.text());
      answerSdp = await sdpResponse.text();
    } catch (e) {
      console.error(e);
      setStatus("OpenAI SDP exchange failed.");
      await stop(); // ensure teardown if failed
      return;
    }

    await pc.setRemoteDescription({ type: "answer", sdp: answerSdp });
    setStatus("Live. Speak to your mic.");
  }

  async function saveConversation() {
    const user_text = userEl.textContent || "";
    const ai_text = aiEl.textContent || "";
    const payload = { session_id: sessionId || "", user_text, ai_text };

    try {
      console.log("Saving conversation...", payload);
      const res = await fetch("/save-conversation/", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        keepalive: true, // helps if page navigates, though we do not navigate here
        body: JSON.stringify(payload),
      });
      const js = await res.json().catch(() => ({}));
      console.log("Save result:", js);
      return js;
    } catch (e) {
      console.warn("Failed to save conversation:", e);
      return null;
    }
  }

  async function stop() {
    stopBtn.disabled = true;
    startBtn.disabled = false;
    setStatus("Stopping...");

    // 1) Save before tearing down WebRTC/mic
    await saveConversation();

    // 2) Tear down
    try { if (eventsDC) eventsDC.close(); } catch {}
    try {
      if (pc) {
        pc.getSenders().forEach(s => { try { s.track && s.track.stop(); } catch {} });
        pc.getReceivers().forEach(r => { try { r.track && r.track.stop(); } catch {} });
        pc.close();
      }
    } catch {}
    if (micStream) { micStream.getTracks().forEach(t => t.stop()); micStream = null; }
    pc = null; eventsDC = null;

    setStatus("Idle");
  }

  startBtn.addEventListener("click", start);
  stopBtn.addEventListener("click", () => { stop(); });
})();
</script>
</body>
</html>
